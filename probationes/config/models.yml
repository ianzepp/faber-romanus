# Model configurations for OpenRouter

# Framework version - bump when tasks, grading logic, or prompts change
framework_version: "1.2"

models:
  # Baseline models
  - id: gpt-3.5-turbo
    provider: openrouter
    name: openai/gpt-3.5-turbo
    cost_per_1m_input: 0.50
    cost_per_1m_output: 1.50

  - id: claude-3-haiku
    provider: openrouter
    name: anthropic/claude-3-haiku
    cost_per_1m_input: 0.25
    cost_per_1m_output: 1.25

  # Small models - testing lower bound
  - id: llama-3.2-1b
    provider: openrouter
    name: meta-llama/llama-3.2-1b-instruct
    cost_per_1m_input: 0.027
    cost_per_1m_output: 0.20

  - id: gemma-2b
    provider: openrouter
    name: google/gemma-3n-e2b-it:free
    cost_per_1m_input: 0.0
    cost_per_1m_output: 0.0

  - id: llama-3.2-3b
    provider: openrouter
    name: meta-llama/llama-3.2-3b-instruct:free
    cost_per_1m_input: 0.0
    cost_per_1m_output: 0.0

  - id: mistral-7b
    provider: openrouter
    name: mistralai/mistral-7b-instruct:free
    cost_per_1m_input: 0.0
    cost_per_1m_output: 0.0

  - id: llama-3.1-8b
    provider: openrouter
    name: meta-llama/llama-3.1-8b-instruct
    cost_per_1m_input: 0.05
    cost_per_1m_output: 0.05

  # Mid-tier models
  - id: gpt-4o-mini
    provider: openrouter
    name: openai/gpt-4o-mini
    cost_per_1m_input: 0.15
    cost_per_1m_output: 0.60

  - id: claude-3.5-sonnet
    provider: openrouter
    name: anthropic/claude-3.5-sonnet
    cost_per_1m_input: 3.00
    cost_per_1m_output: 15.00

  - id: llama-3.1-70b
    provider: openrouter
    name: meta-llama/llama-3.1-70b-instruct
    cost_per_1m_input: 0.35
    cost_per_1m_output: 0.40

  # Large models
  - id: gpt-4o
    provider: openrouter
    name: openai/gpt-4o
    cost_per_1m_input: 2.50
    cost_per_1m_output: 10.00

  - id: claude-3-opus
    provider: openrouter
    name: anthropic/claude-3-opus
    cost_per_1m_input: 15.00
    cost_per_1m_output: 75.00

  # Claude 4.5 series (Anthropic)
  - id: claude-4.5-haiku
    provider: openrouter
    name: anthropic/claude-4.5-haiku
    cost_per_1m_input: 0.80
    cost_per_1m_output: 4.00

  - id: claude-4.5-sonnet
    provider: openrouter
    name: anthropic/claude-4.5-sonnet
    cost_per_1m_input: 3.00
    cost_per_1m_output: 15.00

  - id: claude-4.5-opus
    provider: openrouter
    name: anthropic/claude-4.5-opus
    cost_per_1m_input: 15.00
    cost_per_1m_output: 75.00

  # GPT-5 series (OpenAI)
  - id: gpt-5
    provider: openrouter
    name: openai/gpt-5
    cost_per_1m_input: 5.00
    cost_per_1m_output: 15.00

  - id: gpt-5-mini
    provider: openrouter
    name: openai/gpt-5-mini
    cost_per_1m_input: 0.30
    cost_per_1m_output: 1.20

  - id: o3
    provider: openrouter
    name: openai/o3
    cost_per_1m_input: 10.00
    cost_per_1m_output: 40.00

  - id: o3-mini
    provider: openrouter
    name: openai/o3-mini
    cost_per_1m_input: 1.10
    cost_per_1m_output: 4.40

  - id: o4-mini
    provider: openrouter
    name: openai/o4-mini
    cost_per_1m_input: 1.10
    cost_per_1m_output: 4.40

  # GLM series (Zhipu AI)
  - id: glm-4-plus
    provider: openrouter
    name: zhipu/glm-4-plus
    cost_per_1m_input: 0.50
    cost_per_1m_output: 0.50

  - id: glm-4-long
    provider: openrouter
    name: zhipu/glm-4-long
    cost_per_1m_input: 0.10
    cost_per_1m_output: 0.10

  - id: glm-4-flash
    provider: openrouter
    name: zhipu/glm-4-flash
    cost_per_1m_input: 0.00
    cost_per_1m_output: 0.00

  # Coding-focused models
  - id: deepseek-v3.1
    provider: openrouter
    name: deepseek/deepseek-chat-v3.1
    cost_per_1m_input: 0.15
    cost_per_1m_output: 0.75

  - id: codestral
    provider: openrouter
    name: mistralai/codestral-2508
    cost_per_1m_input: 0.30
    cost_per_1m_output: 0.90

  - id: qwen3-coder
    provider: openrouter
    name: qwen/qwen3-coder
    cost_per_1m_input: 0.22
    cost_per_1m_output: 0.95

  - id: qwen2.5-coder-32b
    provider: openrouter
    name: qwen/qwen-2.5-coder-32b-instruct
    cost_per_1m_input: 0.03
    cost_per_1m_output: 0.11

  - id: mercury-coder
    provider: openrouter
    name: inception/mercury-coder
    cost_per_1m_input: 0.25
    cost_per_1m_output: 1.00

# Trial configuration
trials:
  n_shots: [0, 1, 3, 10]
  dialects: [latin]  # english, symbols later
  contexts: [examples-only, grammar-only, minimal, basic, complete]  # documentation levels
  temperature: 0.0
  max_tokens: 1000
  seed: 42
